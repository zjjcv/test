import os
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from tqdm import tqdm

# =========================================================================
# 0. é…ç½®ä¸Žè·¯å¾„
# =========================================================================
PROJECT_ROOT = "/data/zjj/test"
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# æƒé‡è·¯å¾„é…ç½®
BASE_CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'results', 'gpt2-large', 'checkpoints', 'x_plus_y')
PLOT_DIR = os.path.join(PROJECT_ROOT, 'results', 'analysis_plots', 'embeddings')
os.makedirs(PLOT_DIR, exist_ok=True)

STEPS = [100, 1000, 10000, 100000]
WD_SETTINGS = ['wd_0.0', 'wd_1.0']
P = 97  # æ¨¡æ•°

# ç»˜å›¾é£Žæ ¼
plt.style.use('seaborn-v0_8-paper')
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 10,
    'axes.labelsize': 10,
    'xtick.labelsize': 8,
    'ytick.labelsize': 8,
    'figure.titlesize': 14
})

# =========================================================================
# 1. æ¨¡åž‹å®šä¹‰ (GPT-2 Large Custom Architecture)
# =========================================================================

class GPT2Decoder(nn.Module):
    def __init__(self, dim=1280, num_layers=36, num_heads=20, num_tokens=99, seq_len=5):
        super().__init__()
        # ä»…å®šä¹‰æˆ‘ä»¬éœ€è¦æå–æƒé‡çš„éƒ¨åˆ†ç»“æž„ï¼Œæ— éœ€å®Œæ•´çš„ forward é€»è¾‘
        self.token_embeddings = nn.Embedding(num_tokens, dim)
        self.position_embeddings = nn.Embedding(seq_len, dim)
        
        # æˆ‘ä»¬éœ€è¦è®¿é—® Block 0 çš„ Attention æƒé‡
        self.blocks = nn.ModuleList([
            nn.ModuleDict({
                'attn': nn.Linear(dim, 3 * dim), # c_attn
                'proj': nn.Linear(dim, dim)      # c_proj
            }) for _ in range(num_layers)
        ])
        
        self.ln_f = nn.LayerNorm(dim)
        self.head = nn.Linear(dim, num_tokens, bias=False)

# =========================================================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šæƒé‡æå–ä¸Žé™ç»´
# =========================================================================

def load_checkpoint_weights(wd, step):
    path = os.path.join(BASE_CHECKPOINT_DIR, wd, f"seed42_step{step}.pt")
    if not os.path.exists(path):
        print(f"[WARN] Checkpoint not found: {path}")
        return None
    
    ckpt = torch.load(path, map_location='cpu')
    sd = ckpt['model_state_dict']
    
    # æå–å…³é”®æƒé‡çŸ©é˜µ
    weights = {}
    
    # 1. Input Embeddings [99, 1280]
    weights['wte'] = sd['token_embeddings.weight'].float().numpy()
    
    # 2. Output Embeddings (LM Head) [99, 1280]
    # æ³¨æ„ï¼šå¦‚æžœ head æƒé‡æ˜¯ç‹¬ç«‹çš„ï¼Œå– head.weightï¼›å¦‚æžœæ˜¯ tiedï¼Œå– wte
    if 'head.weight' in sd:
        weights['lm_head'] = sd['head.weight'].float().numpy()
    else:
        weights['lm_head'] = weights['wte'] # Weight Tying
        
    # 3. Layer 0 Attention Key Projection
    # è¿™æ˜¯ä¸€ä¸ª [1280, 3840] çš„çŸ©é˜µã€‚ç›´æŽ¥é™ç»´å®ƒæ²¡æœ‰ç‰©ç†æ„ä¹‰ï¼ˆé‚£æ˜¯ç¥žç»å…ƒç»´åº¦ï¼‰ã€‚
    # ä¸ºäº†å±•ç¤º Grokkingï¼Œæˆ‘ä»¬å°† Input Embedding æŠ•å½±è¿‡ Layer 0 çš„ Key çŸ©é˜µã€‚
    # è¿™ä»£è¡¨äº† "æ¨¡åž‹ç¬¬ä¸€å±‚æ˜¯å¦‚ä½• 'çœ‹' è¿™äº› Token çš„"ã€‚
    # c_attn weight shape is [out, in] in PyTorch Linear, but [in, out] in GPT2 Conv1D usually.
    # Check shape:
    c_attn_w = sd['blocks.0.attn.c_attn.weight'] # Linear: [3*1280, 1280]
    
    # æ‹†åˆ† Q, K, V
    dim = 1280
    # Linear layer weight is (out, in), so (3840, 1280)
    # We want K: slice the middle part
    W_K = c_attn_w[dim:2*dim, :].t() # [1280, 1280]
    
    # è®¡ç®— Projected Embeddings: E @ W_K
    # è¿™å±•ç¤ºäº† Token åœ¨ Attention ç©ºé—´çš„å‡ ä½•ç»“æž„
    wte_tensor = sd['token_embeddings.weight'].float()
    attn_proj = torch.matmul(wte_tensor, W_K).numpy()
    weights['attn_L0_K'] = attn_proj
    
    return weights

def compute_projections(matrix, method='pca'):
    # åªå–å‰ 97 ä¸ª Token (0-96 æ˜¯æ•°å­—)ï¼Œå¿½ç•¥ OP å’Œ EQ
    valid_data = matrix[:P, :] 
    
    if method == 'pca':
        reducer = PCA(n_components=2)
        proj = reducer.fit_transform(valid_data)
        # å½’ä¸€åŒ–ä»¥ä¾¿ç»˜å›¾
        proj = (proj - proj.mean(0)) / (proj.std(0) + 1e-6)
        return proj
    
    elif method == 'tsne':
        # Perplexity è®¾ä¸º 30 æˆ–æ›´å°ï¼ˆå› ä¸ºåªæœ‰ 97 ä¸ªç‚¹ï¼‰
        # FIX: Removed n_iter=1000 since it is default and caused error
        reducer = TSNE(n_components=2, perplexity=20, random_state=42, init='pca', learning_rate='auto')
        proj = reducer.fit_transform(valid_data)
        # å½’ä¸€åŒ–
        proj = (proj - proj.mean(0)) / (proj.std(0) + 1e-6)
        return proj

# =========================================================================
# 3. ç»˜å›¾ä¸»ç¨‹åº
# =========================================================================

def main():
    print("ðŸš€ Starting PCA & t-SNE Analysis for Weights...")
    
    components = ['wte', 'lm_head', 'attn_L0_K']
    titles = ['Input Embeddings ($W_E$)', 'Output Embeddings ($W_U$)', 'Layer 0 Key Proj ($W_E W_K^0$)']
    
    methods = ['pca', 'tsne']
    
    # æˆ‘ä»¬ä¸ºæ¯ä¸ª Component + Method ç»„åˆç”Ÿæˆä¸€å¼ å¤§å›¾
    # å›¾ç»“æž„ï¼š2è¡Œ (WD=0, WD=1) x 4åˆ— (Steps)
    
    for comp_key, comp_title in zip(components, titles):
        for method in methods:
            print(f"ðŸ‘‰ Processing {comp_title} - {method.upper()}...")
            
            fig, axes = plt.subplots(2, 4, figsize=(20, 10), dpi=150)
            plt.subplots_adjust(wspace=0.2, hspace=0.3)
            
            # é¢œè‰²æ˜ å°„ï¼šæ ¹æ®æ•°å­—çš„å€¼ (0-96) ç€è‰²ï¼Œçº¢->ç´«
            colors = np.arange(P)
            
            for row, wd in enumerate(WD_SETTINGS):
                for col, step in enumerate(STEPS):
                    ax = axes[row, col]
                    
                    weights = load_checkpoint_weights(wd, step)
                    if weights is None:
                        ax.text(0.5, 0.5, "Missing", ha='center')
                        continue
                        
                    data = weights[comp_key]
                    proj = compute_projections(data, method=method)
                    
                    # æ•£ç‚¹å›¾
                    scatter = ax.scatter(
                        proj[:, 0], proj[:, 1], 
                        c=colors, 
                        cmap='hsv', 
                        s=40, 
                        alpha=0.8, 
                        edgecolors='grey', 
                        linewidth=0.5
                    )
                    
                    # è¿žçº¿ï¼šå¦‚æžœæ˜¯ Grokkingï¼Œæ•°å­—åº”è¯¥å½¢æˆé—­çŽ¯ã€‚
                    # ç”»ä¸€æ¡æ·¡æ·¡çš„çº¿è¿žæŽ¥ 0->1->...->96->0 è¾…åŠ©è§‚å¯Ÿæ‹“æ‰‘ç»“æž„
                    ax.plot(
                        np.append(proj[:, 0], proj[0, 0]), 
                        np.append(proj[:, 1], proj[0, 1]), 
                        c='gray', alpha=0.3, linestyle='--'
                    )
                    
                    ax.set_title(f"Step {step}", fontsize=12, fontweight='bold')
                    ax.set_xticks([])
                    ax.set_yticks([])
                    
                    # å·¦ä¾§æ ‡æ³¨ WD
                    if col == 0:
                        wd_label = "No Reg (WD=0)" if "0.0" in wd else "Reg (WD=1)"
                        ax.set_ylabel(wd_label, fontsize=14, fontweight='bold')

            # æ·»åŠ  Colorbar
            cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])
            cbar = fig.colorbar(scatter, cax=cbar_ax)
            cbar.set_label('Token Value (0-96)', fontsize=12)
            
            fig.suptitle(f"{comp_title} - {method.upper()} Projection\n(Evolution of Representation Geometry)", fontsize=16, y=0.95)
            
            save_name = f"{comp_key}_{method}.png"
            save_path = os.path.join(PLOT_DIR, save_name)
            plt.savefig(save_path, bbox_inches='tight')
            print(f"   Saved to {save_path}")
            plt.close()

    print("\nâœ… All projections completed.")

if __name__ == "__main__":
    main()